{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ab4962-b65e-4fd0-bf6a-d7d40fb9e544",
   "metadata": {},
   "source": [
    "## Text Mining Project \n",
    "#### In this project, I will be exploring *text message data* and creating models to predict if a message is `spam` or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51896aae-857d-4e16-a36c-3218e2167370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('assets/spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0ac4f7-a57e-4d25-accc-c15ff64e9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e894b2-ff03-42bb-bf4b-4a8cb38701b6",
   "metadata": {},
   "source": [
    "### The percentage of the documents in `spam_data` that are spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eda4db7-4ba5-45f7-8d70-ad8aa0ac48a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_zero():\n",
    "    return spam_data.target.value_counts(normalize=True).iloc[1] * 100\n",
    "step_zero()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cab287-67d2-4bf2-a903-d551e4bb0df4",
   "metadata": {},
   "source": [
    "### Fitting the training data `X_train` using a Count Vectorizer \n",
    "##### then getting the longest token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7ab035-dff4-44dd-a7fc-19c42c193d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def step_one():\n",
    "\n",
    "    vocabulary = CountVectorizer().fit(X_train).vocabulary_\n",
    "    vocabulary = [x for x in vocabulary.keys()]\n",
    "    len_vocabulary = [len(x) for x in vocabulary]\n",
    "\n",
    "    return vocabulary[np.argmax(len_vocabulary)]\n",
    "\n",
    "step_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81182e5a-4aa6-4b69-9f55-b3d58c98c0be",
   "metadata": {},
   "source": [
    "### Fitting a multinomial Naive Bayes classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a51a323-133f-4941-95e3-0ec1aab86193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991545422134696"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def step_two():\n",
    "\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    \n",
    "    # Transforming the training data into a document-term matrix\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    # Calling a Multinomial Naive Bayes classifier\n",
    "    model = MultinomialNB(alpha = 0.1)\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predicting the transformed test documents\n",
    "    prediction = model.predict_proba(vect.transform(X_test))[:, 1]\n",
    "    \n",
    "    # AUC curve\n",
    "    a = roc_auc_score(y_test, prediction)\n",
    "    return a \n",
    "\n",
    "step_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5799a0-b245-492c-81a0-b2e6f506052a",
   "metadata": {},
   "source": [
    "### Fiting and transforming the training data `X_train` using a Tfidf Vectorizer\n",
    "\n",
    "What 20 features have the smallest tf-idf and what 20 have the largest tf-idf among the **max** tf-idf values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5730abe0-f995-4c29-a25b-63069f6c1cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sympathetic     0.074475\n",
       " healer          0.074475\n",
       " aaniye          0.074475\n",
       " dependable      0.074475\n",
       " companion       0.074475\n",
       " listener        0.074475\n",
       " athletic        0.074475\n",
       " exterminator    0.074475\n",
       " psychiatrist    0.074475\n",
       " pest            0.074475\n",
       " determined      0.074475\n",
       " chef            0.074475\n",
       " courageous      0.074475\n",
       " stylist         0.074475\n",
       " psychologist    0.074475\n",
       " organizer       0.074475\n",
       " pudunga         0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64,\n",
       " 146tf150p    1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " okie         1.000000\n",
       " thanx        1.000000\n",
       " er           1.000000\n",
       " anything     1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " yup          1.000000\n",
       " thank        1.000000\n",
       " ok           1.000000\n",
       " where        1.000000\n",
       " beerage      1.000000\n",
       " anytime      1.000000\n",
       " too          1.000000\n",
       " done         1.000000\n",
       " 645          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def step_four():\n",
    "\n",
    "    tf = TfidfVectorizer().fit(X_train)\n",
    "    feature_names = np.array(tf.get_feature_names())\n",
    "    X_train_tf = tf.transform(X_train)\n",
    "    max_tf_idfs = X_train_tf.max(0).toarray()[0]\n",
    "    sorted_tf_idxs = max_tf_idfs.argsort()\n",
    "    sorted_tf_idfs = max_tf_idfs[sorted_tf_idxs]\n",
    "    smallest_tf_idfs = pd.Series(sorted_tf_idfs[:20], index=feature_names[sorted_tf_idxs[:20]])                    \n",
    "    largest_tf_idfs = pd.Series(sorted_tf_idfs[-20:][::-1], index=feature_names[sorted_tf_idxs[-20:][::-1]])\n",
    "\n",
    "    return (smallest_tf_idfs, largest_tf_idfs) \n",
    "step_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9445991-dea2-4c07-96dd-62f88728b650",
   "metadata": {},
   "source": [
    "### Fitting and transforming the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly < **3**.\n",
    "\n",
    "Then fitting a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31ec8ea-dbe5-4ffb-8b27-22bbdeb89941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954968337775665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_four():\n",
    "\n",
    "    vect = TfidfVectorizer(min_df = 3).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    \n",
    "    model = MultinomialNB(alpha = 0.1)\n",
    "    \n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    prediction = model.predict_proba(vect.transform(X_test))[:, 1]\n",
    "\n",
    "    return roc_auc_score(y_test, prediction) \n",
    "\n",
    "step_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a307684-cc3e-4d63-af47-b116f95aef48",
   "metadata": {},
   "source": [
    "### Average length of documents (number of characters) for not spam and spam documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832fc365-9f86-4ce0-a580-28d63dc8088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_five():\n",
    "    spam_data['length'] = spam_data['text'].apply(lambda x:len(x))\n",
    "    return  (np.mean(spam_data['length'][spam_data['target'] == 0]), np.mean(spam_data['length'][spam_data['target'] == 1])) \n",
    "step_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246552c-9f2b-45ab-af8e-16a46684f65e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This function has been used to combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00338fd9-a0ab-484d-b0cc-2832cdf333e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc674381-2a4c-43fd-aa12-858016b10802",
   "metadata": {},
   "source": [
    "### Fitting a Support Vector Classification Model \n",
    "using the document-term matrix and an additional feature (**the length of document (number of characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80caa3f1-c768-4c61-870d-f994c910674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963202213809143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def step_six():\n",
    "\n",
    "    vect = TfidfVectorizer(min_df = 5).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    \n",
    "    #the length of document\n",
    "    len_train = [len(x) for x in X_train]\n",
    "    len_test = [len(x) for x in X_test]\n",
    "    \n",
    "    #adding features\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, len_train)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, len_test)\n",
    "    \n",
    "    # svc model\n",
    "    model = SVC(probability=True, C=10000)\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    prediction = model.predict_proba(X_test_vectorized)[:, 1]\n",
    "\n",
    "    return roc_auc_score(y_test, prediction)\n",
    "\n",
    "step_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8172198-a62a-44b6-a8c9-fdb499032b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average number of digits per document for not spam and spam documents\n",
    "\n",
    "def step_seven():\n",
    "\n",
    "    spam_data['digit_length'] = spam_data.text.apply(lambda x: len([dig for dig in x if dig.isdigit()]))\n",
    "    avg_cat_wise = spam_data.groupby('target').mean()\n",
    "    \n",
    "    return (avg_cat_wise.digit_length.iloc[0], avg_cat_wise.digit_length.iloc[1])\n",
    "\n",
    "step_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079b75f-8105-42f7-9356-7ec03cabdb86",
   "metadata": {},
   "source": [
    "### Fitting a logistic regression model\n",
    "\n",
    "Fiting and transforming the training data `X_train` using the Tfidf Vectorizer ignoring terms that have a document frequency < **5** and using **word n-grams from n=1 to n=3** \n",
    "\n",
    "Using the document-term matrix and the 2 following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "200347f1-e4b3-46df-8fea-86ee454dd436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978651342036908"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def step_eight():\n",
    "\n",
    "    vect = TfidfVectorizer(min_df = 5, ngram_range = (1,3)).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    \n",
    "    #counting digits in text\n",
    "    digits_train = [X_train.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))]\n",
    "    digits_test = [X_test.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))]\n",
    "    \n",
    "    #the length of document\n",
    "    len_train = [len(x) for x in X_train]\n",
    "    len_test = [len(x) for x in X_test]\n",
    "    \n",
    "    #adding features\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, len_train)\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, digits_train)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, len_test)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, digits_test)\n",
    "    \n",
    "    # logistic model\n",
    "    model = LogisticRegression(C=100)\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    prediction = model.predict_proba(X_test_vectorized)[:, 1]\n",
    "\n",
    "    return roc_auc_score(y_test, prediction)\n",
    "\n",
    "step_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081567f6-ca9d-45e5-8bfe-9b475a842924",
   "metadata": {},
   "source": [
    "### Average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc49c22c-f91a-4b79-8c17-18bc8bf59823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_nine():\n",
    "    return (np.mean(spam_data.loc[spam_data['target']==0,'text'].str.count('\\W')), \n",
    "            np.mean(spam_data.loc[spam_data['target']==1,'text'].str.count('\\W')))\n",
    "step_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857046d-9150-4d32-869a-81e46a924ad8",
   "metadata": {},
   "source": [
    "### V2 of Fitting a logistic regression model \n",
    "\n",
    "This time, using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "##### *After the first iteration and new learnings, I updated this model by passing in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This makes the model more robust to spelling mistakes.*\n",
    "\n",
    "The new set of features : \n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3475a9a7-9d39-4fb1-bb11-2544d04a824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996642785596659"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_ten():\n",
    "\n",
    "    vect = CountVectorizer(min_df=5, analyzer='char_wb', ngram_range=[2,5])\n",
    "    \n",
    "    X_train_vect = vect.fit_transform(X_train)\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    X_train_vect_with_length = add_feature(X_train_vect, [X_train.str.len()\n",
    "                                                          , X_train.apply(lambda x: len([dig for dig in x if dig.isdigit()]))\n",
    "                                                          , X_train.str.findall(r'(\\W)').str.len()])\n",
    "    X_test_vect_with_length = add_feature(X_test_vect, [X_test.str.len()\n",
    "                                                          , X_test.apply(lambda x: len([dig for dig in x if dig.isdigit()]))\n",
    "                                                          , X_test.str.findall(r'(\\W)').str.len()])\n",
    "    \n",
    "    clfLG = LogisticRegression(C=100)\n",
    "    clfLG.fit(X_train_vect_with_length, y_train)\n",
    "    y_predicted = clfLG.predict_proba(X_test_vect_with_length)[:, 1]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "    return (auc_score)\n",
    "\n",
    "step_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66a52b-b7b1-4c63-9b8d-77d56a720a3f",
   "metadata": {},
   "source": [
    "## Potential Limitations of the project\n",
    "* Despite using techniques like document frequency filtering, there's still a risk of overfitting\n",
    "* The model's effectiveness might be constrained to the language in which it was trained ( English )\n",
    "\n",
    "I welcome any feedback about how to improve the robustness of my model and address any of the project's limitations other than the listed ones above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c765af0-e361-46d4-b6b9-56753d3215b2",
   "metadata": {},
   "source": [
    "**Thank You for Reviewing This Project**\n",
    "\n",
    "I appreciate you taking the time to go through my work. Please feel free to reach out if you have any questions, suggestions, or would like to discuss any aspects of this project further.\n",
    "\n",
    "Best Regards,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d19a55-9ff3-462c-b053-263586e9d7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
